import os
import streamlit as st
from llamafactory.chat import ChatModel
from llamafactory.extras.misc import torch_gc

# ØªØºÛŒÛŒØ± Ù…Ø³ÛŒØ± Ø¨Ù‡ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ Ù…Ø¯Ù„
os.chdir("/LLaMA-Factory")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ú©Ø´
@st.cache_resource
def load_model():
    args = dict(
        model_name_or_path="unsloth/llama-3-8b-Instruct-bnb-4bit",
        # adapter_name_or_path="llama3_lora",
        adapter_name_or_path="llama3_lora_identity",
        # adapter_name_or_path="llama3_lora_identity_final",
        template="llama3",
        finetuning_type="lora",
    )
    return ChatModel(args)

chat_model = load_model()

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡
st.set_page_config(page_title="Ú†Øª Ø¨Ø§ LLaMA 3", layout="wide")

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø³ØªØ§ÛŒÙ„ Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†
st.markdown("""
    <style>
        body, .stApp {
            direction: rtl;
            text-align: right;
        }
        .stTextInput input {
            direction: rtl;
            text-align: right;
        }
        .stChatMessage {
            direction: rtl;
            text-align: right;
        }
        .css-1cpxqw2, .css-12oz5g7 {
            direction: rtl;
            text-align: right;
        }
    </style>
""", unsafe_allow_html=True)

# Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡
st.title("ğŸ¦™ Ú†Øª Ø¨Ø§ Ù…Ø¯Ù„ LLaMA 3")

# Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
if "messages" not in st.session_state:
    st.session_state.messages = []

# Ù†ÙˆØ§Ø± Ú©Ù†Ø§Ø±ÛŒ
with st.sidebar:
    st.header("ğŸ›  ØªÙ†Ø¸ÛŒÙ…Ø§Øª")
    if st.button("ğŸ—‘ Ù¾Ø§Ú©â€ŒÚ©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡"):
        st.session_state.messages = []
        torch_gc()
        st.success("ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾Ø§Ú© Ø´Ø¯.")

# ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø±
user_input = st.chat_input("Ù¾ÛŒØ§Ù… Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
    with st.chat_message("user"):
        st.markdown(user_input)

    # Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
    with st.chat_message("assistant"):
        response = ""
        response_placeholder = st.empty()
        for chunk in chat_model.stream_chat(st.session_state.messages):
            response += chunk
            response_placeholder.markdown(response + "â–Œ")
        response_placeholder.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

# Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ
torch_gc()
